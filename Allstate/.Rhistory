plot(lasso_model, xvar="lambda", label=TRUE) # I like 3
lasso_pred3 <- as.numeric(predict(lasso_model, s = 3, newx  = x)  )
plot_pred(lasso_pred3)
x <- as.data.frame(x)
testX <- as.data.frame(testX)
x <- cbind(trainId, y, x)
x <- filter(x, y < 35000)
y <- x$y
trainId <- x$trainId
x$y <- x$trainId <- NULL
#convert back into a matrix to run through the LASSO
x <- as.matrix(x)
testX <- as.matrix(testX)
lasso_model <- glmnet(x, y, alpha = 1)
plot(lasso_model, xvar="lambda", label=TRUE) # I like 3
lasso_pred3 <- as.numeric(predict(lasso_model, s = 3, newx  = x)  )
plot_pred(lasso_pred3)
#lets check the lasso with log(y)
lasso_model <- glmnet(x, log(y), alpha = 1)
plot(lasso_model, xvar="lambda", label=TRUE)
lasso_pred_log <- as.numeric(exp(predict(lasso_model, s = -5, newx  = x)))
plot_pred(lasso_pred_log)
17522-1700
mae <- function(y, pred)  mean(abs(pred - y))
cv_score_function <- "mae"    #What CV error are we minimizing in trainOneFold?
setwd("/home/chasedehan/Dropbox/R Code/ML/Allstate")
library(plyr)
library(tidyverse)
library(caret)
#library(doMC)
#registerDoMC(cores=8)
##################### Prepping Data #############################
prepFeatures <- function(train, test, idName, yName){
#Comgine the train/test and prep features
test[, yName] <- NA
y <- c(train[, yName], test[, yName])
id <- c(train[, idName], test[, idName])
pred <- rbind(train, test)
pred[, idName] <- pred[, yName] <- NULL
pred <- as.data.frame(model.matrix(~ ., pred)[, -1])
#Split the data back up
train <- list(y = y[!is.na(y)],
id = id[!is.na(y)],
predictors = pred[!is.na(y), ])
test <- list(id = id[is.na(y)],
predictors = pred[is.na(y), ])
#return a list
return(list(train = train, test = test))
}
#Set the data into a list
L0Features <- prepFeatures(as.data.frame(read_csv("train.csv")),
as.data.frame(read_csv("test.csv")),
"id", "loss")
################ Training the model #######
trainOneFold <- function(this_fold, feature_set, method, tuning_params = NULL) {
#Make sure that all important parameters are passed as a list of arguments for "other_param"
# get fold specific cv data
cv.data <- list()
cv.data$predictors <- feature_set$train$predictors[this_fold,]
cv.data$ID <- feature_set$train$id[this_fold]
cv.data$y <- feature_set$train$y[this_fold]
# get training data for specific fold
train.data <- list()
train.data$predictors <- feature_set$train$predictors[-this_fold,]
train.data$y <- feature_set$train$y[-this_fold]
#Fit the model for the fold
fitted_mdl <- train(x=train.data$predictors,
y=train.data$y,
method = method,
tuneGrid = tuning_params)
yhat <- predict(fitted_mdl,newdata = cv.data$predictors,type = "raw")
score <- get(cv_score_function)(cv.data$y,yhat)
ans <- list(fitted_mdl=fitted_mdl,
score=score,
predictions=data.frame(ID=cv.data$ID,yhat=yhat,y=cv.data$y))
return(ans)
}
########### Training the model: general parameters ###############
mae <- function(y, pred)  mean(abs(pred - y))
cv_score_function <- "mae"    #What CV error are we minimizing in trainOneFold?
data_folds <- createFolds(L0Features$train$y, k=5)
set.seed(567)
ten_percent <- sample(1:nrow(L0Features$train$y), 0.1 * nrow(L0Features$train$y))
ten_percent
L0Features$train$y
ten_percent <- sample(1:length(L0Features$train$y), 0.1 * length(L0Features$train$y))
ten_percent
mdl <- trainOneFold(ten_percent, L0Features, method = "xgbTree")
ggplot(mdl)
mdl
mdl$fitted_mdl
ggplot(mdl$fitted_mdl)
mdl$fitted_mdl$bestTune
36000/.2
36000/.2*4
grid_tune_param<- expand.grid(nrounds = (2:12) * 25,
max_depth = c(3, 5, 7, 9, 11)
eta = 0.3,
gamma = 0,
grid_tune_param<- expand.grid(nrounds = (2:12) * 25,
max_depth = c(3, 5, 7, 9, 11),
eta = 0.3,
gamma = 0,
colsample_bytree = 0.6,
min_child_weight = 1)
data_folds <- createFolds(L0Features$train$y, k=5)
rm(mdl)
mdl2 <- llply(data_folds,trainOneFold,L0Features, method = "xgbTree", tuning_params = grid_tune_param)
ggplot(mdl2$Fold1$fitted_mdl)
ggplot(mdl2$Fold2$fitted_mdl)
mdl2$Fold2$fitted_mdl$bestTune
ggplot(mdl2$Fold3$fitted_mdl)
ggplot(mdl2$Fold4$fitted_mdl)
ggplot(mdl2$Fold5$fitted_mdl)
grid_tune_param<- expand.grid(nrounds = (3:12) * 30,
max_depth = c(3, 4, 5),
eta = 0.3,
gamma = 0,
colsample_bytree = 0.6,
min_child_weight = 1)
######Then run the model to tune the model
mdl2 <- llply(data_folds,trainOneFold,L0Features, method = "xgbTree", tuning_params = grid_tune_param)
ggplot(mdl2$Fold5$fitted_mdl)
ggplot(mdl2$Fold4$fitted_mdl)
ggplot(mdl2$Fold3$fitted_mdl)
ggplot(mdl2$Fold2$fitted_mdl)
ggplot(mdl2$Fold1$fitted_mdl)
new_grid <- mdl %>%
ldply(function(x){ x$fitted_mdl$bestTune} )[, -1]
new_grid <- mdl2 %>%
ldply(function(x){ x$fitted_mdl$bestTune} )[, -1]
mdl2$Fold1$fitted_mdl$bestTune
ldply(mdl2, function(x){ x$fitted_mdl$bestTune} )[, -1]
new_grid <-  ldply(mdl2, function(x){ x$fitted_mdl$bestTune} )[, -1]
new_grid %>% colMeans() %>%
rbind() %>%
as.data.frame()
tuned_mdl <- trainOneFold(ten_percent,
L0Features,
method = "xgbTree",
tuning_params = new_grid)
new_grid
new_grid <-  ldply(mdl2, function(x){ x$fitted_mdl$bestTune} )[, -1]
new_grid <- new_grid %>%
colMeans() %>%
rbind() %>%
as.data.frame()
new_grid
class(new_grid)
new_grid[2] <- 4
new_grid
gc()
tuned_mdl <- train(x=L0Features$train$predictors,
y=L0Features$train$y,
method = method,
tuneGrid = new_grid)
tuned_mdl <- train(x=L0Features$train$predictors,
y=L0Features$train$y,
method = "xgbTree",
tuneGrid = new_grid)
tuned_mdl$modelInfo
tuned_mdl
test_yhat <- predict(tuned_mdl, newdata = L0FeatureSet1$test$predictors, type = "raw")
test_yhat <- predict(tuned_mdl, newdata = L0Features$test$predictors, type = "raw")
head(test_yhat)
getwd()
submit <- function(x) {
submission <- data.frame(id = testId, loss = x)
print(head(submission))
write.csv(submission, file = "allstate_submission.csv", row.names = FALSE)
}
submit(test_yhat)
submit <- function(x) {
submission <- data.frame(id = L0Features$test$id, loss = x)
print(head(submission))
write.csv(submission, file = "allstate_submission.csv", row.names = FALSE)
}
submit(test_yhat)
new_grid
train_yhat <- predict(tuned_mdl, newdata = L0Features$train$predictors, type = "raw")
plot(L0Features$train$y, train_yhat)
abline(0,1, col="red")
plot(L0Features$train$y, train_yhat)
abline(0,1, col="red")
mae <- function(y, pred)  mean(abs(pred - y))
mae(L0Features$train$y, train_yhat)
head(train_yhat)
?summaryFunction
?trainControl
twoClassSummary()
twoClassSummary
?requireNamespaceQuietStop
requireNamespaceQuietStop
18*30
rm(tuned_mdl)
setwd("/home/chasedehan/Dropbox/R Code/ML/Allstate")
library(plyr)
library(tidyverse)
library(caret)
#library(doMC)
#registerDoMC(cores=8)
##################### Prepping Data #############################
prepFeatures <- function(train, test, idName, yName){
#Comgine the train/test and prep features
test[, yName] <- NA
y <- log(c(train[, yName], test[, yName]) )
id <- c(train[, idName], test[, idName])
pred <- rbind(train, test)
pred[, idName] <- pred[, yName] <- NULL
pred <- as.data.frame(model.matrix(~ ., pred)[, -1])
#Split the data back up
train <- list(y = y[!is.na(y)],
id = id[!is.na(y)],
predictors = pred[!is.na(y), ])
test <- list(id = id[is.na(y)],
predictors = pred[is.na(y), ])
#return a list
return(list(train = train, test = test))
}
#Set the data into a list
L0Features <- prepFeatures(as.data.frame(read_csv("train.csv")),
as.data.frame(read_csv("test.csv")),
"id", "loss")
trainOneFold <- function(this_fold, feature_set, method, tuning_params = NULL, metric = NULL, maximize = NULL) {
#Make sure that all important parameters are passed as a list of arguments for "other_param"
# get fold specific cv data
cv.data <- list()
cv.data$predictors <- feature_set$train$predictors[this_fold,]
cv.data$ID <- feature_set$train$id[this_fold]
cv.data$y <- feature_set$train$y[this_fold]
# get training data for specific fold
train.data <- list()
train.data$predictors <- feature_set$train$predictors[-this_fold,]
train.data$y <- feature_set$train$y[-this_fold]
#Fit the model for the fold
fitted_mdl <- train(x=train.data$predictors,
y=train.data$y,
method = method,
tuneGrid = tuning_params,
metric = metric,
maximize = maximize)
yhat <- predict(fitted_mdl,newdata = cv.data$predictors,type = "raw")
score <- get(cv_score_function)(cv.data$y,yhat)
ans <- list(fitted_mdl=fitted_mdl,
score=score,
predictions=data.frame(ID=cv.data$ID,yhat=yhat,y=cv.data$y))
return(ans)
}
mae <- function(y, pred)  mean(abs(pred - y))
cv_score_function <- "mae"    #What CV error are we minimizing in trainOneFold?
maeSummary <- function(data, lev = NULL, model = NULL) {
out <- mean(abs(data$pred - data$obs))
names(out) <- "MAE"
out
}
################ Training the model #######
trainOneFold <- function(this_fold, feature_set, method, tuning_params = NULL, trControl = NULL, metric = NULL, maximize = NULL) {
#Make sure that all important parameters are passed as a list of arguments for "other_param"
# get fold specific cv data
cv.data <- list()
cv.data$predictors <- feature_set$train$predictors[this_fold,]
cv.data$ID <- feature_set$train$id[this_fold]
cv.data$y <- feature_set$train$y[this_fold]
# get training data for specific fold
train.data <- list()
train.data$predictors <- feature_set$train$predictors[-this_fold,]
train.data$y <- feature_set$train$y[-this_fold]
#Fit the model for the fold
fitted_mdl <- train(x=train.data$predictors,
y=train.data$y,
method = method,
tuneGrid = tuning_params,
trControl = trControl,
metric = metric,
maximize = maximize)
yhat <- predict(fitted_mdl,newdata = cv.data$predictors,type = "raw")
score <- get(cv_score_function)(cv.data$y,yhat)
ans <- list(fitted_mdl=fitted_mdl,
score=score,
predictions=data.frame(ID=cv.data$ID,yhat=yhat,y=cv.data$y))
return(ans)
}
setwd("/home/chasedehan/Dropbox/R Code/ML/Allstate")
library(plyr)
library(tidyverse)
library(caret)
#library(doMC)
#registerDoMC(cores=8)
##################### Prepping Data #############################
prepFeatures <- function(train, test, idName, yName){
#Comgine the train/test and prep features
test[, yName] <- NA
y <- log(c(train[, yName], test[, yName]) )
id <- c(train[, idName], test[, idName])
pred <- rbind(train, test)
pred[, idName] <- pred[, yName] <- NULL
pred <- as.data.frame(model.matrix(~ ., pred)[, -1])
#Split the data back up
train <- list(y = y[!is.na(y)],
id = id[!is.na(y)],
predictors = pred[!is.na(y), ])
test <- list(id = id[is.na(y)],
predictors = pred[is.na(y), ])
#return a list
return(list(train = train, test = test))
}
#Set the data into a list
L0Features <- prepFeatures(as.data.frame(read_csv("train.csv")),
as.data.frame(read_csv("test.csv")),
"id", "loss")
################ Training the model #######
trainOneFold <- function(this_fold, feature_set, method, tuning_params = NULL, trControl = NULL, metric = NULL, maximize = NULL) {
#Make sure that all important parameters are passed as a list of arguments for "other_param"
# get fold specific cv data
cv.data <- list()
cv.data$predictors <- feature_set$train$predictors[this_fold,]
cv.data$ID <- feature_set$train$id[this_fold]
cv.data$y <- feature_set$train$y[this_fold]
# get training data for specific fold
train.data <- list()
train.data$predictors <- feature_set$train$predictors[-this_fold,]
train.data$y <- feature_set$train$y[-this_fold]
#Fit the model for the fold
fitted_mdl <- train(x=train.data$predictors,
y=train.data$y,
method = method,
tuneGrid = tuning_params,
trControl = trControl,
metric = metric,
maximize = maximize)
yhat <- predict(fitted_mdl,newdata = cv.data$predictors,type = "raw")
score <- get(cv_score_function)(cv.data$y,yhat)
ans <- list(fitted_mdl=fitted_mdl,
score=score,
predictions=data.frame(ID=cv.data$ID,yhat=yhat,y=cv.data$y))
return(ans)
}
########### Training the model: general parameters ###############
mae <- function(y, pred)  mean(abs(pred - y))
cv_score_function <- "mae"    #What CV error are we minimizing in trainOneFold?
#First go through of data using 10% of data
set.seed(567)
ten_percent <- sample(1:length(L0Features$train$y), 0.1 * length(L0Features$train$y))
mdl <- trainOneFold(ten_percent, L0Features, method = "xgbTree")
grid_tune_param<- expand.grid(nrounds = (2:15) * 30,
max_depth = 3:7,
eta = c(0.05, 0.075, 0.1, 0.125, 0.2),
gamma = 0,
colsample_bytree = 0.7,
min_child_weight = 1)
nrows(grid_tune_param)
dim(grid_tune_param)
maeSummary <- function(data, lev = NULL, model = NULL) {
out <- mean(abs(data$pred - data$obs))
names(out) <- "MAE"
out
}
ctrl <- trainControl(summaryFunction = maeSummary)
data_folds <- createFolds(L0Features$train$y, k=5)
mdl2 <- llply(data_folds,trainOneFold,L0Features, method = "xgbTree",
tuning_params = grid_tune_param,
trControl = ctrl,
metric = "MAE",
maximize = FALSE)
ggplot(mdl2$Fold1$fitted_mdl)
new_grid <-  ldply(mdl2, function(x){ x$fitted_mdl$bestTune} )[, -1]
new_grid
gc()
9*50
750/50
cv.score <- mean(ldply(mdl2, function(x){ x$score})[, -1])
cv.score
exp(cv.score)
log(cv.score)
lapply(mdl2, function(x){ ggplot(x$fitted_mdl})
lapply(mdl2, function(x){ ggplot(x$fitted_mdl)})
grid_tune_param<- expand.grid(nrounds = (9:17) * 50,
max_depth = c(7,9,11),
eta = 0.05,
gamma = 0,
colsample_bytree = 0.7,
min_child_weight = 1)
data_folds <- createFolds(L0Features$train$y, k=5)
setwd("/home/chasedehan/Dropbox/R Code/ML/Allstate")
library(plyr)
library(tidyverse)
library(caret)
#library(doMC)
#registerDoMC(cores=8)
##################### Prepping Data #############################
prepFeatures <- function(train, test, idName, yName){
#Comgine the train/test and prep features
test[, yName] <- NA
y <- log(c(train[, yName], test[, yName]) )
id <- c(train[, idName], test[, idName])
pred <- rbind(train, test)
pred[, idName] <- pred[, yName] <- NULL
pred <- as.data.frame(model.matrix(~ ., pred)[, -1])
#Split the data back up
train <- list(y = y[!is.na(y)],
id = id[!is.na(y)],
predictors = pred[!is.na(y), ])
test <- list(id = id[is.na(y)],
predictors = pred[is.na(y), ])
#return a list
return(list(train = train, test = test))
}
#Set the data into a list
L0Features <- prepFeatures(as.data.frame(read_csv("train.csv")),
as.data.frame(read_csv("test.csv")),
"id", "loss")
################ Training the model #######
trainOneFold <- function(this_fold, feature_set, method, tuning_params = NULL, trControl = NULL, metric = NULL, maximize = NULL) {
#Make sure that all important parameters are passed as a list of arguments for "other_param"
# get fold specific cv data
cv.data <- list()
cv.data$predictors <- feature_set$train$predictors[this_fold,]
cv.data$ID <- feature_set$train$id[this_fold]
cv.data$y <- feature_set$train$y[this_fold]
# get training data for specific fold
train.data <- list()
train.data$predictors <- feature_set$train$predictors[-this_fold,]
train.data$y <- feature_set$train$y[-this_fold]
#Fit the model for the fold
fitted_mdl <- train(x=train.data$predictors,
y=train.data$y,
method = method,
tuneGrid = tuning_params,
trControl = trControl,
metric = metric,
maximize = maximize)
yhat <- predict(fitted_mdl,newdata = cv.data$predictors,type = "raw")
score <- get(cv_score_function)(cv.data$y,yhat)
ans <- list(fitted_mdl=fitted_mdl,
score=score,
predictions=data.frame(ID=cv.data$ID,yhat=yhat,y=cv.data$y))
return(ans)
}
########### Training the model: general parameters ###############
mae <- function(y, pred)  mean(abs(pred - y))
cv_score_function <- "mae"    #What CV error are we minimizing in trainOneFold?
#First go through of data using 10% of data
#set.seed(567)
#ten_percent <- sample(1:length(L0Features$train$y), 0.1 * length(L0Features$train$y))
#mdl <- trainOneFold(ten_percent, L0Features, method = "xgbTree")
#Look at the data to see how well it does
#ggplot(mdl)
#Select the appropriate tuning parameters for the expanded grid search
grid_tune_param<- expand.grid(nrounds = (9:17) * 50,
max_depth = c(7,9,11),
eta = 0.05,
gamma = 0,
colsample_bytree = 0.7,
min_child_weight = 1)
#Define the parameters to min/max
maeSummary <- function(data, lev = NULL, model = NULL) {
out <- mean(abs(data$pred - data$obs))
names(out) <- "MAE"
out
}
ctrl <- trainControl(summaryFunction = maeSummary)
######Then run the model to tune the model
data_folds <- createFolds(L0Features$train$y, k=5)
mdl2 <- llply(data_folds,trainOneFold,L0Features, method = "xgbTree",
tuning_params = grid_tune_param,
trControl = ctrl,
metric = "MAE",
maximize = FALSE)
#Repeat if necessary
lapply(mdl2, function(x){ ggplot(x$fitted_mdl)})
new_grid <-  ldply(mdl2, function(x){ x$fitted_mdl$bestTune} )[, -1]
new_grid
new_grid <- new_grid %>%
colMeans() %>%
rbind() %>%
as.data.frame()
new_grid
tuned_mdl <- train(x=L0Features$train$predictors,
y=L0Features$train$y,
method = "xgbTree",
tuneGrid = new_grid)
train_yhat <- predict(tuned_mdl, newdata = L0Features$train$predictors, type = "raw")
plot(train_yhat, L0Features$train$y)
abline(0,1, col = "red")
plot(exp(train_yhat), exp(L0Features$train$y))
abline(0,1, col = "red")
test_yhat <- exp(predict(tuned_mdl, newdata = L0Features$test$predictors, type = "raw") )
submit <- function(x) {
submission <- data.frame(id = L0Features$test$id, loss = x)
print(head(submission))
write.csv(submission, file = "allstate_submission.csv", row.names = FALSE)
}
submit(test_yhat)
new_grid
dim(L0Features$train$predictors)
rm(tuned_mdl)
gc()
mdl2 <- llply(data_folds,trainOneFold,L0Features, method = "xgbTree",
tuning_params = new_grid,
trControl = ctrl,
metric = "MAE",
maximize = FALSE)
rm(mdl2)
gc()
mdl2 <- llply(data_folds,trainOneFold,L0Features, method = "xgbTree",
tuning_params = new_grid,
trControl = ctrl,
metric = "MAE",
maximize = FALSE)
#Check how the model performs
cv.score <- mean(ldply(mdl2, function(x){ x$score})[, -1])
cv.score
