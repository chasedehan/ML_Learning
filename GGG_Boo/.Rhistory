install.packages("data.table")
library(plyr)
library(tidyverse)
library(caret)
#library(doMC)
#registerDoMC(cores=8)
setwd("/home/chasedehan/Dropbox/R Code/ML/GGG_Boo")
train_data <- read_csv("train.csv")
test_data <- read_csv("test.csv")
#Prepping the dataframe
#combine to make all_data
test_data$type <- NA
all_data <- rbind(train_data, test_data)
type <- as.factor(all_data$type)
all_data <- as.data.frame(model.matrix(~ ., all_data[, -7]))[, -1]
all_data <- cbind(type, all_data)
head(all_data)
#Split back up
train_data <- all_data %>% filter(!is.na(type))
test_data <- all_data %>% filter(is.na(type))
test_data$type <- NULL
############### Prepping the feature lists ###############
prepL0Features_cat <- function(df){
id <- df[, response_Id]
if (class(df$type) != "NULL") {  #WONKY, it works for the other data, hardcoded here
y <- as.factor(df[, response_col_name])
df[, response_Id] <- df[, response_col_name] <- NULL
} else {
y <- NULL
df[, response_Id] <- NULL
}
return(list(id=id,y=y,predictors=df))
}
################ Training the model #######
trainOneFold <- function(this_fold, feature_set, other_param) {
#Make sure that all important parameters are passed as a list of arguments for "other_param"
# get fold specific cv data
cv.data <- list()
cv.data$predictors <- feature_set$train$predictors[this_fold,]
cv.data$ID <- feature_set$train$id[this_fold]
cv.data$y <- feature_set$train$y[this_fold]
# get training data for specific fold
train.data <- list()
train.data$predictors <- feature_set$train$predictors[-this_fold,]
train.data$y <- feature_set$train$y[-this_fold]
#Fit the model for the fold
fitted_mdl <- do.call(train,
c(list(x=train.data$predictors,y=train.data$y),
verbose = 0,
other_param) )
yhat <- predict(fitted_mdl,newdata = cv.data$predictors,type = "raw")
score <- get(cv_score_function)(cv.data$y,yhat)
ans <- list(fitted_mdl=fitted_mdl,
score=score,
predictions=data.frame(ID=cv.data$ID,yhat=yhat,y=cv.data$y))
return(ans)
}
#####################  Setting up the data #######################
#Name these to prep the data
response_Id <- "id"       #Name of the id in the dataframe
response_col_name <- 'type'    #Name of the response variable NOT PASSING CORRECTLY RIGHT NOW
#Build the features
L0FeatureSet1 <- list(train=prepL0Features_cat(train_data),
test=prepL0Features_cat(test_data))
########### Training the model: general parameters ###############
accuracy <- function(y, yhat) {
conf <- table(y, yhat)
acc <- sum(diag(conf)) / sum(conf)
return(acc)
}
cv_score_function <- "accuracy"   #What measure are we trying to measure in CV
data_folds <- createFolds(L0FeatureSet1$train$y, k=5)  #creating the folds
L0_data <- list(x=L0FeatureSet1$train$predictors,y=L0FeatureSet1$train$y)
#Could probably compartmentalize the below a little better
################################################
############# XGB Model ########################
################################################
grid_tune_param<- NULxpand.grid(nrounds = (1:20) * 10,
max_depth = 1,
eta = c(0.2, 0.3, 0.4),
gamma = 0,
colsample_bytree = c(0.4, 0.6, 0.8),
min_child_weight = 1)
model_param <- list(method = "xgbTree", grid_tune_param)
grid_tune_param<- expand.grid(nrounds = (1:20) * 10,
max_depth = 1,
eta = c(0.2, 0.3, 0.4),
gamma = 0,
colsample_bytree = c(0.4, 0.6, 0.8),
min_child_weight = 1)
model_param <- list(method = "xgbTree", grid_tune_param)
######Then run the model to tune the model
xgb_set <- llply(data_folds,trainOneFold,L0FeatureSet1, model_param)
model_param <- list(method = "xgbTree")
xgb_set <- llply(data_folds,trainOneFold,L0FeatureSet1, model_param)
a <- ldply(xgb_set, function(x){ x$fitted_mdl$bestTune} )[, -1]
a
dim(a)
grid_tune_param<- expand.grid(nrounds = (1:20) * 10,
max_depth = 1,
eta = c(0.2, 0.3, 0.4),
gamma = 0,
colsample_bytree = c(0.4, 0.6, 0.8),
min_child_weight = 1)
model_param <- list(method = "xgbTree", grid_tune_param)
######Then run the model to tune the model
xgb_set <- llply(data_folds,trainOneFold,L0FeatureSet1, model_param)
grid_tune_param
class(grid_tune_param)
a
lapply(xgb_set, function(x){ ggplot(x$fitted_mdl) })
names(a)
names(grid_tune_param)
names(grid_tune_param) == names(a)
#Name these to prep the data
response_Id <- "id"       #Name of the id in the dataframe
response_col_name <- 'type'    #Name of the response variable NOT PASSING CORRECTLY RIGHT NOW
#Build the features
L0FeatureSet1 <- list(train=prepL0Features_cat(train_data),
test=prepL0Features_cat(test_data))
########### Training the model: general parameters ###############
accuracy <- function(y, yhat) {
conf <- table(y, yhat)
acc <- sum(diag(conf)) / sum(conf)
return(acc)
}
cv_score_function <- "accuracy"   #What measure are we trying to measure in CV
data_folds <- createFolds(L0FeatureSet1$train$y, k=5)  #creating the folds
L0_data <- list(x=L0FeatureSet1$train$predictors,y=L0FeatureSet1$train$y)
#Could probably compartmentalize the below a little better
################################################
############# XGB Model ########################
################################################
grid_tune_param<- expand.grid(nrounds = (1:20) * 10,
max_depth = 1,
eta = c(0.2, 0.3, 0.4),
gamma = 0,
colsample_bytree = c(0.4, 0.6, 0.8),
min_child_weight = 1)
model_param <- list(method = "xgbTree", grid_tune_param)
######Then run the model to tune the model
xgb_set <- llply(data_folds,trainOneFold,L0FeatureSet1, model_param)
a <- ldply(xgb_set, function(x){ x$fitted_mdl$bestTune} )[, -1]
a
c <- colMeans(a)
c
d <- rbind(c)
d
class(d)
d <- as.data.frame(rbind(c))
d
class(d)
model_param <- list(method = "xgbTree", d)
######Then run the model to tune the model
xgb_set <- llply(data_folds,trainOneFold,L0FeatureSet1, model_param)
L0FeatureSet1 <- list(train=prepL0Features_cat(train_data),
test=prepL0Features_cat(test_data))
response_Id <- "id"       #Name of the id in the dataframe
response_col_name <- 'type'    #Name of the response variable NOT PASSING CORRECTLY RIGHT NOW
#Build the features
L0FeatureSet1 <- list(train=prepL0Features_cat(train_data),
test=prepL0Features_cat(test_data))
########### Training the model: general parameters ###############
accuracy <- function(y, yhat) {
conf <- table(y, yhat)
acc <- sum(diag(conf)) / sum(conf)
return(acc)
}
cv_score_function <- "accuracy"   #What measure are we trying to measure in CV
data_folds <- createFolds(L0FeatureSet1$train$y, k=5)  #creating the folds
L0_data <- list(x=L0FeatureSet1$train$predictors,y=L0FeatureSet1$train$y)
#Could probably compartmentalize the below a little better
################################################
############# XGB Model ########################
################################################
grid_tune_param<- expand.grid(nrounds = (1:20) * 10,
max_depth = 1,
eta = c(0.2, 0.3, 0.4),
gamma = 0,
colsample_bytree = c(0.4, 0.6, 0.8),
min_child_weight = 1)
model_param <- list(method = "xgbTree", d)
######Then run the model to tune the model
xgb_set <- llply(data_folds,trainOneFold,L0FeatureSet1, model_param)
grid_tune_param<-  NULL  #use expand.grid() to build the model parameters
model_param <- list(method = "ranger", tuneGrid = grid_tune_param)
######Then run the model
rf_set <- llply(data_folds,trainOneFold,L0FeatureSet1, model_param)
cv.score <- mean(do.call(c, lapply(xgb_set, function(x){x$score})))
cv.score  #0.7252252 accuracy
cv.score <- mean(do.call(c, lapply(rf_set, function(x){x$score})))
cv.score <- mean(do.call(c, lapply(rf_set, function(x){x$score})))
trainOneFold <- function(this_fold, feature_set, other_param) {
#Make sure that all important parameters are passed as a list of arguments for "other_param"
# get fold specific cv data
cv.data <- list()
cv.data$predictors <- feature_set$train$predictors[this_fold,]
cv.data$ID <- feature_set$train$id[this_fold]
cv.data$y <- feature_set$train$y[this_fold]
# get training data for specific fold
train.data <- list()
train.data$predictors <- feature_set$train$predictors[-this_fold,]
train.data$y <- feature_set$train$y[-this_fold]
#Fit the model for the fold
fitted_mdl <- do.call(train,
c(list(x=train.data$predictors,y=train.data$y),
verbose = 0,
other_param) )
yhat <- predict(fitted_mdl,newdata = cv.data$predictors,type = "raw")
score <- get(cv_score_function)(cv.data$y,yhat)
ans <- list(fitted_mdl=fitted_mdl,
score=score,
predictions=data.frame(ID=cv.data$ID,yhat=yhat,y=cv.data$y))
return(ans)
}
library(plyr)
library(tidyverse)
library(caret)
setwd("/home/chasedehan/Dropbox/R Code/ML/GGG_Boo")
train_data <- read_csv("train.csv")
test_data <- read_csv("test.csv")
test_data$type <- NA
all_data <- rbind(train_data, test_data)
type <- as.factor(all_data$type)
all_data <- as.data.frame(model.matrix(~ ., all_data[, -7]))[, -1]
all_data <- cbind(type, all_data)
head(all_data)
#Split back up
train_data <- all_data %>% filter(!is.na(type))
test_data <- all_data %>% filter(is.na(type))
test_data$type <- NULL
############### Prepping the feature lists ###############
prepL0Features_cat <- function(df){
id <- df[, response_Id]
if (class(df$type) != "NULL") {  #WONKY, it works for the other data, hardcoded here
y <- as.factor(df[, response_col_name])
df[, response_Id] <- df[, response_col_name] <- NULL
} else {
y <- NULL
df[, response_Id] <- NULL
}
return(list(id=id,y=y,predictors=df))
}
################ Training the model #######
trainOneFold <- function(this_fold, feature_set, other_param) {
#Make sure that all important parameters are passed as a list of arguments for "other_param"
# get fold specific cv data
cv.data <- list()
cv.data$predictors <- feature_set$train$predictors[this_fold,]
cv.data$ID <- feature_set$train$id[this_fold]
cv.data$y <- feature_set$train$y[this_fold]
# get training data for specific fold
train.data <- list()
train.data$predictors <- feature_set$train$predictors[-this_fold,]
train.data$y <- feature_set$train$y[-this_fold]
#Fit the model for the fold
fitted_mdl <- do.call(train,
c(list(x=train.data$predictors,y=train.data$y),
verbose = 0,
other_param) )
yhat <- predict(fitted_mdl,newdata = cv.data$predictors,type = "raw")
score <- get(cv_score_function)(cv.data$y,yhat)
ans <- list(fitted_mdl=fitted_mdl,
score=score,
predictions=data.frame(ID=cv.data$ID,yhat=yhat,y=cv.data$y))
return(ans)
}
response_Id <- "id"       #Name of the id in the dataframe
response_col_name <- 'type'    #Name of the response variable NOT PASSING CORRECTLY RIGHT NOW
L0FeatureSet1 <- list(train=prepL0Features_cat(train_data),
test=prepL0Features_cat(test_data))
accuracy <- function(y, yhat) {
conf <- table(y, yhat)
acc <- sum(diag(conf)) / sum(conf)
return(acc)
}
cv_score_function <- "accuracy"   #What measure are we trying to measure in CV
data_folds <- createFolds(L0FeatureSet1$train$y, k=5)  #creating the folds
L0_data <- list(x=L0FeatureSet1$train$predictors,y=L0FeatureSet1$train$y)
grid_tune_param<- expand.grid(nrounds = (1:20) * 10,
max_depth = 1,
eta = c(0.2, 0.3, 0.4),
gamma = 0,
colsample_bytree = c(0.4, 0.6, 0.8),
min_child_weight = 1)
model_param <- list(method = "xgbTree", grid_tune_param)
xgb_set <- llply(data_folds,trainOneFold,L0FeatureSet1, model_param)
train(y = L0FeatureSet1$train$y, x = L0FeatureSet1$train$predictors, method = "xgbTree")
?train
grid_tune_param<- expand.grid(nrounds = (1:20) * 10,
max_depth = 1,
eta = c(0.2, 0.3, 0.4),
gamma = 0,
colsample_bytree = c(0.4, 0.6, 0.8),
min_child_weight = 1)
train(y = L0FeatureSet1$train$y, x = L0FeatureSet1$train$predictors, method = "xgbTree", tuneGrid = grid_tune_param)
model_param <- list(method = "xgbTree", grid_tune_param)
train(y = L0FeatureSet1$train$y, x = L0FeatureSet1$train$predictors, model_param)
model_param <- list(method = "xgbTree", grid_tune_param)
model_param
model_param <- list(method = "xgbTree", grid_tune_param = NULL)
model_param
model_param <- list(method = "xgbTree", tuneGrid = grid_tune_param)
train(y = L0FeatureSet1$train$y, x = L0FeatureSet1$train$predictors, model_param)
model_param <- list(method = "xgbTree", tuneGrid = grid_tune_param)
train(y = L0FeatureSet1$train$y, x = L0FeatureSet1$train$predictors, method = "xgbTree", grid_tune_param)
train(y = L0FeatureSet1$train$y, x = L0FeatureSet1$train$predictors, method = "xgbTree", tuneGrid = grid_tune_param)
grid_tune_param<- expand.grid(nrounds = (1:20) * 10,
max_depth = 1,
eta = c(0.2, 0.3, 0.4),
gamma = 0,
colsample_bytree = c(0.4, 0.6, 0.8),
min_child_weight = 1)
model_param <- list(method = "xgbTree", tuneGrid = grid_tune_param)
xgb_set <- llply(data_folds,trainOneFold,L0FeatureSet1, model_param)
lapply(xgb_set, function(x){ ggplot(x$fitted_mdl) })
cv.score <- mean(do.call(c, lapply(xgb_set, function(x){x$score})))
cv.score  #0.7252252 accuracy
a <- ldply(xgb_set, function(x){ x$fitted_mdl$bestTune} )[, -1]
a
c <- colMeans(a)
c
d <- as.data.frame(rbind(c))
d
model_param <- list(method = "xgbTree", tuneGrid = d)
xgb_set <- llply(data_folds,trainOneFold,L0FeatureSet1, model_param)
####### And check the output of the model
cv.score <- mean(do.call(c, lapply(xgb_set, function(x){x$score})))
xgb_set$Fold1$score
dlply(xgb_set, function(x){x$score})
xgb_set$Fold1$score
xgb_set$Fold2$score
xgb_set$Fold3$score
xgb_set$Fold4$score
xgb_set$Fold5$score
lapply(xgb_set, function(x){ x$score})
cv.score <- mean(do.call(c, lapply(xgb_set, function(x){ x$score})))
lapply(xgb_set, function(x){ x$score})
do.call(c, lapply(xgb_set, function(x){ x$score}))
ldply(xgb_set, function(x){ x$score})
cv.score <- mean(ldply(xgb_set, function(x){ x$score}))
cv.score <- mean(ldply(xgb_set, function(x){ x$score})[, -1])
cv.score  #0.7141622 accuracy
xgb_mdl <- do.call(train, c(L0_data, verbose = 0, model_param) )
xgb_test_yhat <- predict(xgb_mdl, newdata = L0FeatureSet1$test$predictors, type = "raw")
ggplot(xgb_mdl)
submit <- function(x) {
submission <- data.frame(id = L1_test_data$id, type = x )
print(head(submission))
write.csv(submission, file = "ggb_submission.csv", row.names = FALSE)
}
submit(xgb_test_yhat)
submit <- function(x) {
submission <- data.frame(id = L0FeatureSet1$test$id, type = x )
print(head(submission))
write.csv(submission, file = "ggb_submission.csv", row.names = FALSE)
}
submit(xgb_test_yhat)
xgb_mdl <- do.call(train, c(L0_data, verbose = 0, method = "xgbTree") )
xgb_test_yhat <- predict(xgb_mdl, newdata = L0FeatureSet1$test$predictors, type = "raw")
ggplot(xgb_mdl)
submit(xgb_test_yhat)
model_param <- list(method = "xgbTree", tuneGrid = NULL)
######Then run the model to tune the model
xgb_set <- llply(data_folds,trainOneFold,L0FeatureSet1, model_param)
cv.score <- mean(ldply(xgb_set, function(x){ x$score})[, -1])
cv.score  #0.7141622 accuracy
